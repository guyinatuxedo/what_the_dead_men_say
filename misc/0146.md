## Description

Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.

get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.
put(key, value) - Set or insert the value if the key is not already present. When the cache reaches its capacity, it should invalidate the least recently used item before inserting a new item.

The cache is initialized with a positive capacity.

Follow up:
Could you do both operations in O(1) time complexity?

```
Example:

LRUCache cache = new LRUCache( 2 /* capacity */ );

cache.put(1, 1);
cache.put(2, 2);
cache.get(1);       // returns 1
cache.put(3, 3);    // evicts key 2
cache.get(2);       // returns -1 (not found)
cache.put(4, 4);    // evicts key 1
cache.get(1);       // returns -1 (not found)
cache.get(3);       // returns 3
cache.get(4);       // returns 4
```

## Java LinkedHashMap

So for this problem, there already exists a data structure that has all of the functionality we need to solve that challenge. For Java this is known as a `LinkedHashMap`. This data structure is effectively a hashmap, that keeps track of ordering of the elements. It does this via keeping a doubly linked list through all of the entries. What this enabled us to do, is leverage the `get` and `put` functionality like a typical hashmap. In addition to that because of the doubly linked list which keeps track of the ordering of accesses, we can easily remove the oldest entry when the time comes.

For making the cache, we will just use the `LinkedHashMap` constructor. For this we will need to specify three arguments. The first is the size of the LinkedHashMap in terms of items it will store. The second is the loadFactor, which is the percentage of slots that will be filled before the hashmap expands it's size. The reason for this being that if the amount of spots in the hash map increases enough, that will increase the number of hash collisions and decrease performance of insertions and lookups. The default is `75%` so we will go with that. Then the final argument will specify if the ordering we care about is either insertion (so eldest entry) or accessing (which is what we care about).

For dealing with `get`, there is a `LinkedHashMap` function called `getOrDefault`. This will lookup a key in a hashMap, and return a default value if it isn't found. This is exactly what we are looking for.

For `put` we can just use the `LinkedHashMap` function `put`.

For dealing with the automatic entry removal of the last accessed node when the capacity gets too high, we will override the `removeEldestEntry()` function. This function automatically runs for a lot of `LinkedHashMap` functions for insertions, and lookups, including the ones we are using. So we just override it to return `True` when our condition is met, which is when the size is greater than our specified capacity, which we store in a private int.

The runtime for insertions, and lookups is both `O(1)` since it is a hashmap, and the LinkedList portion makes the automatic removal operations (both removing it and keeping track of which thing should be removed) also `O(1)`. The memory cost however is `O(n)`, since we need to keep that many items in the `LinkedHashMap`.

```
class LRUCache extends LinkedHashMap<Integer, Integer>
{
    private int maxCapacity;
    
    public LRUCache(int capacity) 
    {
        // use the lhm constructor
        super(capacity, 0.75F, true);
        
        // store our max capacity
        this.maxCapacity = capacity;
    }
    
    // Essentially use the getOrDefault lhm function
    public int get(int key) 
    {
        return super.getOrDefault(key, -1);
    }

    // Essentially use the put lhm function
    public void put(int key, int value) 
    {
        super.put(key, value);
    }
    
    // Override the removeEldestEntry function to suit our need
    @Override
    protected boolean removeEldestEntry(Map.Entry<Integer, Integer> eldest)
    {
        return size() > maxCapacity;
    }
}

/**
 * Your LRUCache object will be instantiated and called as such:
 * LRUCache obj = new LRUCache(capacity);
 * int param_1 = obj.get(key);
 * obj.put(key,value);
 */
```

## Python OrderedDict

This solution is almost exactly the same as the earlier solution. The main difference is that this is in python, and uses an `OrderedDict`, which is pretty much the same. One thing about an `OrderedDict` is that it keeps track of items automatically as they are inserted, so we need to manually add code to update the ordering for accesses too.

The memory / runtime costs are the same as the previous solution.

```
from collections import OrderedDict

class LRUCache(OrderedDict):
    def __init__(self, capacity: int):
        # Set the capacity
        self.capacity = capacity

    def get(self, key: int) -> int:
        # Check if key exists
        if key not in self:
            return -1
        
        # Move key to last spot in list
        self.move_to_end(key)
        
        # Return the value
        return self[key]

    def put(self, key: int, value: int) -> None:
        # If key exists, move the item to the most recently accessed
        if key in self:
            self.move_to_end(key)
        
        # Insert the value
        self[key] = value
        
        # Pop the lru value, if the cache is too big
        if len(self) > self.capacity:
            self.popitem(last = False)

# Your LRUCache object will be instantiated and called as such:
# obj = LRUCache(capacity)
# param_1 = obj.get(key)
# obj.put(key,value)
```